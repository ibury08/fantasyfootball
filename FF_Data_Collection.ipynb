{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named constraint",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f7db91d944bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mconstraint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named constraint"
     ]
    }
   ],
   "source": [
    "import cellbell\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from constraint import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "with open('config.json') as f:\n",
    "    conf = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection & Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trying the database route\n",
    "conn_str = \"host={} dbname={} user={} password={}\".format(conf['host'],conf['database'], conf['user'], conf['passw'])\n",
    "conn = psycopg2.connect(conn_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plays = pd.read_sql('select * from play_player ', con=conn)\n",
    "games = pd.read_sql('select * from game ', con=conn)\n",
    "players = pd.read_sql('select * from player ', con=conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teams = pd.read_sql('select * from team',con=conn)\n",
    "teams.to_csv('/Users/ianbury/Springboard/FantasyFootball/teams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teams = pd.read_csv('/Users/ianbury/Springboard/FantasyFootball/teams.csv')\n",
    "teams.drop('Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping non-grouping columns + defensive fields. Might create a defensive scoring dataframe later.\n",
    "def_cols = []\n",
    "for d in plays.columns.values.tolist():\n",
    "    if d.startswith('defense'):\n",
    "        def_cols.append(d)\n",
    "\n",
    "def_cols.extend(('drive_id','play_id'))\n",
    "plays_dropped = plays.drop(columns=def_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Aggregating stats to game-player level\n",
    "plays_sum = plays_dropped.groupby(by=['gsis_id','player_id','team']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fixing inconsistent complete of name columns\n",
    "def fix_names(row):\n",
    "    \"\"\"This function looks up the chain of various name columns to fix null column issues\"\"\"\n",
    "    if not row['gsis_name']:\n",
    "        if not row['full_name']:\n",
    "            if not row['last_name']:\n",
    "                    return None\n",
    "            else:\n",
    "                if not row['first_name']:\n",
    "                    return row['first_name'][0]+'.'+row['last_name']\n",
    "                else:\n",
    "                    return row['last_name']\n",
    "        else:\n",
    "            return row['full_name']\n",
    "    else:\n",
    "        return row['gsis_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Applying fix_names function from above to create a new column\n",
    "players.loc[:,'name_fixed'] = players.apply(fix_names,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping extra name columns and other unneccessary columns now\n",
    "players_dropped = players[['name_fixed','player_id','position','years_pro','height','weight']]\n",
    "#bring back previous school? Don't drop too early. Do some statistical digging first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#joining players to aggregated plays\n",
    "pp = plays_sum.merge(players_dropped, on='player_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping unnecessary columns from games\n",
    "games_dropped = games[['gsis_id','start_time','week','day_of_week','season_year','season_type','home_team','away_team','home_score','away_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#joining players-plays to games\n",
    "ppg = pp.merge(games_dropped, on='gsis_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Export data as a checkpoint\n",
    "#ppg.to_csv('/Users/ianbury/player-game-stats.csv')\n",
    "%ding\n",
    "#could use pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>gsis_id</th>\n",
       "      <th>player_id</th>\n",
       "      <th>team</th>\n",
       "      <th>fumbles_forced</th>\n",
       "      <th>fumbles_lost</th>\n",
       "      <th>fumbles_notforced</th>\n",
       "      <th>fumbles_oob</th>\n",
       "      <th>fumbles_rec</th>\n",
       "      <th>fumbles_rec_tds</th>\n",
       "      <th>...</th>\n",
       "      <th>weight</th>\n",
       "      <th>start_time</th>\n",
       "      <th>week</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>season_year</th>\n",
       "      <th>season_type</th>\n",
       "      <th>home_team</th>\n",
       "      <th>away_team</th>\n",
       "      <th>home_score</th>\n",
       "      <th>away_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2009080950</td>\n",
       "      <td>00-0003292</td>\n",
       "      <td>BUF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>247.0</td>\n",
       "      <td>2009-08-10</td>\n",
       "      <td>0</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2009</td>\n",
       "      <td>Preseason</td>\n",
       "      <td>TEN</td>\n",
       "      <td>BUF</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2009080950</td>\n",
       "      <td>00-0009056</td>\n",
       "      <td>BUF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>265.0</td>\n",
       "      <td>2009-08-10</td>\n",
       "      <td>0</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2009</td>\n",
       "      <td>Preseason</td>\n",
       "      <td>TEN</td>\n",
       "      <td>BUF</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2009080950</td>\n",
       "      <td>00-0011626</td>\n",
       "      <td>BUF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>174.0</td>\n",
       "      <td>2009-08-10</td>\n",
       "      <td>0</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2009</td>\n",
       "      <td>Preseason</td>\n",
       "      <td>TEN</td>\n",
       "      <td>BUF</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2009080950</td>\n",
       "      <td>00-0012478</td>\n",
       "      <td>BUF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>224.0</td>\n",
       "      <td>2009-08-10</td>\n",
       "      <td>0</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2009</td>\n",
       "      <td>Preseason</td>\n",
       "      <td>TEN</td>\n",
       "      <td>BUF</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2009080950</td>\n",
       "      <td>00-0019310</td>\n",
       "      <td>BUF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>227.0</td>\n",
       "      <td>2009-08-10</td>\n",
       "      <td>0</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2009</td>\n",
       "      <td>Preseason</td>\n",
       "      <td>TEN</td>\n",
       "      <td>BUF</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     gsis_id   player_id team  fumbles_forced  fumbles_lost  \\\n",
       "0           0  2009080950  00-0003292  BUF               0             0   \n",
       "1           1  2009080950  00-0009056  BUF               0             0   \n",
       "2           2  2009080950  00-0011626  BUF               0             0   \n",
       "3           3  2009080950  00-0012478  BUF               0             0   \n",
       "4           4  2009080950  00-0019310  BUF               0             0   \n",
       "\n",
       "   fumbles_notforced  fumbles_oob  fumbles_rec  fumbles_rec_tds     ...      \\\n",
       "0                  0            0            0                0     ...       \n",
       "1                  0            0            0                0     ...       \n",
       "2                  0            0            0                0     ...       \n",
       "3                  0            0            0                0     ...       \n",
       "4                  0            0            0                0     ...       \n",
       "\n",
       "   weight  start_time  week  day_of_week  season_year  season_type  home_team  \\\n",
       "0   247.0  2009-08-10     0       Sunday         2009    Preseason        TEN   \n",
       "1   265.0  2009-08-10     0       Sunday         2009    Preseason        TEN   \n",
       "2   174.0  2009-08-10     0       Sunday         2009    Preseason        TEN   \n",
       "3   224.0  2009-08-10     0       Sunday         2009    Preseason        TEN   \n",
       "4   227.0  2009-08-10     0       Sunday         2009    Preseason        TEN   \n",
       "\n",
       "   away_team  home_score  away_score  \n",
       "0        BUF          21          18  \n",
       "1        BUF          21          18  \n",
       "2        BUF          21          18  \n",
       "3        BUF          21          18  \n",
       "4        BUF          21          18  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading from checkpoint\n",
    "ppg = pd.read_csv('/Users/ianbury/Springboard/FantasyFootball/player-game-stats.csv', parse_dates=['start_time'])\n",
    "teams = pd.read_csv('/Users/ianbury/Springboard/FantasyFootball/teams.csv')\n",
    "teams.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "ppg.head()\n",
    "#could use min-max scaler on height/weight/year_pro (important for K-mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translating Stats to Fantasy Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Might add a config file here to set your league's settings in the future\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding list of relevant columns to scoring\n",
    "scoring_cols = ['fantasy_score',\n",
    "                'fumbles_rec_tds',\n",
    "                'fumbles_lost',\n",
    "                'passing_int',\n",
    "                'passing_yds',\n",
    "                'passing_tds',\n",
    "                'passing_twoptm',\n",
    "                'receiving_rec',\n",
    "                'receiving_tds',\n",
    "                'receiving_twoptm',\n",
    "                'receiving_yds',\n",
    "                'rushing_yds',\n",
    "                'rushing_tds',\n",
    "                'rushing_twoptm',\n",
    "                'kicking_rec_tds',\n",
    "                'puntret_tds',\n",
    "                'receiving_tar',\n",
    "                'rushing_att',\n",
    "                'passing_att'\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of relevant non-scoring columns\n",
    "feature_cols=['gsis_id',\n",
    " 'player_id',\n",
    " 'team',\n",
    " 'name_fixed',\n",
    " 'position',\n",
    " 'years_pro',\n",
    " 'height',\n",
    " 'weight',\n",
    " 'start_time',\n",
    " 'week',\n",
    " 'day_of_week',\n",
    " 'season_year',\n",
    " 'season_type',\n",
    " 'home_team',\n",
    " 'away_team',\n",
    " 'home_score',\n",
    " 'away_score']\n",
    "#add back in some of the dropped features from previous cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(row):\n",
    "    \"\"\"This function take all the columns relevant for scoring and apply their point values, \n",
    "    resulting in a fantasy score for each player-game\"\"\"\n",
    "    passing = row['passing_int']*(-1)+row['passing_yds']*0.04+row['passing_tds']*4+row['passing_twoptm']*2\n",
    "    rushing = row['rushing_yds']*0.1+row['rushing_twoptm']*2+row['rushing_tds']*6\n",
    "    receiving = row['receiving_twoptm']*2+row['receiving_tds']*6+row['receiving_yds']*0.1+row['receiving_rec']*0.5\n",
    "    fumbles = row['fumbles_rec_tds']*6+row['fumbles_lost']*(-2)\n",
    "    returns = row['kicking_rec_tds']*6+row['puntret_tds']*6\n",
    "   \n",
    "    score = passing+rushing+receiving+fumbles+returns\n",
    "    \n",
    "\n",
    "    return score\n",
    "#could leave per-category (per stat line) predictions\n",
    "#could produce confidence interval as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying scoring\n",
    "ppg['fantasy_score']=ppg.apply(scoring,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping unnecessary columns\n",
    "ppg = ppg[feature_cols+scoring_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>home_team</th>\n",
       "      <th>away_team</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>133026</th>\n",
       "      <td>2014-11-02 18:00:00</td>\n",
       "      <td>KC</td>\n",
       "      <td>NYJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153335</th>\n",
       "      <td>2015-10-11 20:05:00</td>\n",
       "      <td>DET</td>\n",
       "      <td>ARI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118009</th>\n",
       "      <td>2014-01-11 21:35:00</td>\n",
       "      <td>SEA</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23647</th>\n",
       "      <td>2010-01-17 21:40:00</td>\n",
       "      <td>SD</td>\n",
       "      <td>NYJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188923</th>\n",
       "      <td>2017-08-09 23:30:00</td>\n",
       "      <td>CAR</td>\n",
       "      <td>HOU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139530</th>\n",
       "      <td>2014-12-20 21:30:00</td>\n",
       "      <td>WAS</td>\n",
       "      <td>PHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56646</th>\n",
       "      <td>2011-09-25 20:15:00</td>\n",
       "      <td>CHI</td>\n",
       "      <td>GB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125410</th>\n",
       "      <td>2014-09-07 20:25:00</td>\n",
       "      <td>DAL</td>\n",
       "      <td>SF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151205</th>\n",
       "      <td>2015-09-27 17:00:00</td>\n",
       "      <td>BAL</td>\n",
       "      <td>CIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150779</th>\n",
       "      <td>2015-09-27 17:00:00</td>\n",
       "      <td>CAR</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81472</th>\n",
       "      <td>2012-09-30 20:25:00</td>\n",
       "      <td>TB</td>\n",
       "      <td>WAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165497</th>\n",
       "      <td>2016-01-10 01:15:00</td>\n",
       "      <td>CIN</td>\n",
       "      <td>PIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89808</th>\n",
       "      <td>2012-12-02 21:25:00</td>\n",
       "      <td>OAK</td>\n",
       "      <td>CLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73041</th>\n",
       "      <td>2012-08-17 00:00:00</td>\n",
       "      <td>ATL</td>\n",
       "      <td>CIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172664</th>\n",
       "      <td>2016-09-11 20:05:00</td>\n",
       "      <td>SEA</td>\n",
       "      <td>MIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                start_time home_team away_team\n",
       "133026 2014-11-02 18:00:00        KC       NYJ\n",
       "153335 2015-10-11 20:05:00       DET       ARI\n",
       "118009 2014-01-11 21:35:00       SEA        NO\n",
       "23647  2010-01-17 21:40:00        SD       NYJ\n",
       "188923 2017-08-09 23:30:00       CAR       HOU\n",
       "139530 2014-12-20 21:30:00       WAS       PHI\n",
       "56646  2011-09-25 20:15:00       CHI        GB\n",
       "125410 2014-09-07 20:25:00       DAL        SF\n",
       "151205 2015-09-27 17:00:00       BAL       CIN\n",
       "150779 2015-09-27 17:00:00       CAR        NO\n",
       "81472  2012-09-30 20:25:00        TB       WAS\n",
       "165497 2016-01-10 01:15:00       CIN       PIT\n",
       "89808  2012-12-02 21:25:00       OAK       CLE\n",
       "73041  2012-08-17 00:00:00       ATL       CIN\n",
       "172664 2016-09-11 20:05:00       SEA       MIA"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We already have day of week and week of schedule, so let's truncate start_time to only be game time HH:MM for early vs. late game as a feature\n",
    "ppg[['start_time', 'home_team','away_team']].sample(n=15)\n",
    "#You'd think the offset would match the home_team timezone but that doesn't look to be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['UNK', 'QB', 'RB', 'DE', 'TE', 'DB', 'WR', 'DT', 'K', 'P', 'LB',\n",
       "       'OT', 'OG', 'NT', 'C', 'LS', 'T', 'FB', 'CB', 'SAF', 'OLB', 'OL',\n",
       "       'ILB'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see all the different positions we have in the set. \n",
    "ppg['position'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Focusing on only offensive positions for now. Also merging FB & RB\n",
    "ppg['position'].replace('FB','RB',inplace=True)\n",
    "off_pos = ['QB','RB','TE','WR']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Preseason and Postseason games due to inconsistent week labels\n",
    "ppg=ppg[ppg['season_type']=='Regular']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dict of dataframes of Fantasy scores and features by game-player by postion\n",
    "pos_scores={}\n",
    "for o in off_pos:\n",
    "    pos_scores[o] = ppg[ppg['position']==o]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternatively, using multi-index\n",
    "ppg_mi = ppg.set_index(['position','season_year','week']).sort_index()\n",
    "mi_sorted = ppg_mi.loc[off_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "season_year  week  season_type\n",
       "2009         1     Regular        16\n",
       "             2     Regular        16\n",
       "             3     Regular        16\n",
       "             4     Regular        14\n",
       "             5     Regular        14\n",
       "             6     Regular        14\n",
       "             7     Regular        13\n",
       "             8     Regular        13\n",
       "             9     Regular        13\n",
       "             10    Regular        15\n",
       "             11    Regular        16\n",
       "             12    Regular        16\n",
       "             13    Regular        16\n",
       "             14    Regular        16\n",
       "             15    Regular        16\n",
       "             16    Regular        16\n",
       "             17    Regular        16\n",
       "2010         1     Regular        16\n",
       "             2     Regular        16\n",
       "             3     Regular        16\n",
       "             4     Regular        14\n",
       "             5     Regular        14\n",
       "             6     Regular        14\n",
       "             7     Regular        14\n",
       "             8     Regular        13\n",
       "             9     Regular        13\n",
       "             10    Regular        14\n",
       "             11    Regular        16\n",
       "             12    Regular        16\n",
       "             13    Regular        16\n",
       "                                  ..\n",
       "2016         5     Regular        14\n",
       "             6     Regular        15\n",
       "             7     Regular        15\n",
       "             8     Regular        13\n",
       "             9     Regular        13\n",
       "             10    Regular        14\n",
       "             11    Regular        14\n",
       "             12    Regular        16\n",
       "             13    Regular        15\n",
       "             14    Regular        16\n",
       "             15    Regular        16\n",
       "             16    Regular        16\n",
       "             17    Regular        16\n",
       "2017         1     Regular        15\n",
       "             2     Regular        16\n",
       "             3     Regular        16\n",
       "             4     Regular        16\n",
       "             5     Regular        14\n",
       "             6     Regular        14\n",
       "             7     Regular        15\n",
       "             8     Regular        13\n",
       "             9     Regular        13\n",
       "             10    Regular        14\n",
       "             11    Regular        14\n",
       "             12    Regular        16\n",
       "             13    Regular        16\n",
       "             14    Regular        16\n",
       "             15    Regular        16\n",
       "             16    Regular        16\n",
       "             17    Regular        16\n",
       "Name: gsis_id, Length: 153, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Something's wrong with the original data around pre-season games and week number not being consistent year-to-year.\n",
    "idx=pd.IndexSlice\n",
    "ppg_mi.loc[idx[:,:,:],['home_team','away_team','season_type','start_time','gsis_id']].groupby(['season_year','week','season_type'])['gsis_id'].nunique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Due to inconsistent week labelling, I'm going to filter our all Postseason & Preseason games.\n",
    "ppg_mi = ppg_mi[ppg_mi['season_type']=='Regular']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping - Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#example_url= 'http://www.nflweather.com/en/week/2014/week-1/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Merge on week-year-hometeam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_base = 'http://www.nflweather.com/en/week/'\n",
    "week=''\n",
    "year=''\n",
    "soup_mix=[]\n",
    "for year in range(2009,2018):\n",
    "    for week in range(1,18):\n",
    "        r=requests.get(url_base+str(year)+'/week-'+str(week))\n",
    "        html_doc = r.text\n",
    "        soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "        df = parse_forecast(soup)\n",
    "        df['Year']=year\n",
    "        df['Week']=week\n",
    "        soup_mix.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather = pd.concat(soup_mix)\n",
    "#checkpoint for weather data, so I don't have to spam the weather url\n",
    "weather.to_csv('/Users/ianbury/Springboard/FantasyFootball/weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Forecasts start on element 9, then every 13 elements.\n",
    "def parse_forecast(soup):\n",
    "    \"\"\"Takes the html output of a BeautifulSoup object, \n",
    "    parses out elements from 5th and 9th columns (home team and weather forecast in this case), and returns a dataframe.\"\"\"\n",
    "    d={}\n",
    "    f=[]\n",
    "    home=[]\n",
    "    for k,v in enumerate(soup.find_all('td')):\n",
    "        if k in range(5,500,13):\n",
    "            home.append(v.text.strip())\n",
    "        if k in range(9,500,13):\n",
    "            f.append(v.string.strip())\n",
    "    d['home']=home\n",
    "    d['forecast']=f\n",
    "    df=pd.DataFrame(d)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading from checkpoint\n",
    "weather=pd.read_csv('/Users/ianbury/Springboard/FantasyFootball/weather.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping St.Louis Rams & San Diego Charges - teams have moved locations\n",
    "teams=teams.drop([22,25],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Merging weather with team names so we can merge with the mega dataframe\n",
    "wt = weather.drop('Unnamed: 0',axis=1).merge(teams,how='left', left_on='home', right_on='name',validate='m:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "565"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's classify the forecasts!\n",
    "wt.forecast.nunique()\n",
    "#A ton of unique values - time for regex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting forecast column into temperature and description\n",
    "regex = re.compile(r'\\d+f ')\n",
    "\n",
    "wt['temp']= wt['forecast'].apply(lambda x: pd.to_numeric(x.split('f')[0].split('/')[0]) if bool(regex.search(x)) else np.nan)\n",
    "wt['desc']=wt['forecast'].apply(lambda x: regex.split(x)[1] if bool(regex.search(x)) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A quick search shows most domes are temperature controlled at 65F.\n",
    "wt.temp.fillna(65,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Mostly Cloudy', 'DOME', 'Fair', 'Partly Cloudy', 'Overcast',\n",
       "       'A Few Clouds', 'Light Rain', 'Light Rain Fog/Mist', 'Fog/Mist',\n",
       "       'Clear', 'Overcast with Haze', 'Rain Fog/Mist', 'Partly Sunny',\n",
       "       'Overcast and Windy', 'Fog', 'Few Clouds', 'Snow',\n",
       "       'Overcast and Breezy', 'A Few Clouds with Haze',\n",
       "       'Mostly Cloudy and Breezy', 'Partly Cloudy with Haze',\n",
       "       'Partly Cloudy and Breezy', 'Fair with Haze', 'Mostly Sunny',\n",
       "       'Rain Showers', 'Light Drizzle', 'Sunny', 'Light Rain and Breezy',\n",
       "       'Light Snow', ' Light Rain', 'Patchy Fog',\n",
       "       '  Thunderstorm in Vicinity Light Rain Fog/Mist', 'NA',\n",
       "       ' Light Drizzle', 'Fair and Breezy', 'Lt Rain',\n",
       "       ' Rain Fog/Mist and Breezy', 'Rain', ' Light Rain Fog/Mist',\n",
       "       ' Fog/Mist', 'Lt Rain, Fog', ' Light Snow Fog/Mist',\n",
       "       ' Blowing Snow and Breezy', ' Thunderstorm Rain Fog/Mist',\n",
       "       'Thunder, Lt Rain', 'Mostly Clear', 'Areas Drizzle', 'Breezy',\n",
       "       ' Snow and Breezy', 'Areas Fog', ' Light Freezing Rain',\n",
       "       'Wintry Mix', ' Heavy Snow Freezing Fog', ' Light Snow',\n",
       "       'A few Clouds', 'Showers', ' Heavy Rain Fog/Mist',\n",
       "       ' Rain Fog/Mist', ' Thunderstorm', '  Showers in Vicinity', ' Fog',\n",
       "       'Cloudy', 'Chance Showers', 'Mod Rain, Fog', 'Drizzle',\n",
       "       'Dry and Partly Cloudy', 'Humid and Mostly Cloudy', 'Humid',\n",
       "       'Foggy', 'Heavy Rain', 'Possible Drizzle', 'Dry',\n",
       "       'Humid and Partly Cloudy', 'Possible Light Rain', 'Flurries'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt.desc.unique()\n",
    "#There's probably a more elegant way to sort weather descriptions into categories - I used a spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wet = [' Light Rain','Light Rain Fog/Mist','Fog/Mist','Rain Fog/Mist','Fog','Rain Showers','Light Drizzle','Light Rain and Breezy','Light Rain','  Thunderstorm in Vicinity Light Rain Fog/Mist',' Light Drizzle','Lt Rain',' Rain Fog/Mist and Breezy','Rain',' Light Rain Fog/Mist',' Fog/Mist','Lt Rain, Fog',' Thunderstorm Rain Fog/Mist','Thunder, Lt Rain','Areas Drizzle','Showers',' Heavy Rain Fog/Mist',' Rain Fog/Mist',' Thunderstorm','  Showers in Vicinity',' Fog','Mod Rain, Fog','Drizzle','Foggy','Heavy Rain','Possible Drizzle']\n",
    "good = ['Mostly Cloudy','DOME','Fair','Partly Cloudy','Overcast','A Few Clouds','Clear','Overcast with Haze','Partly Sunny','Few Clouds','A Few Clouds with Haze','Partly Cloudy with Haze','Fair with Haze','Mostly Sunny','Sunny','Patchy Fog','NA','Mostly Clear','Areas Fog','A few Clouds','Cloudy','Chance Showers','Dry and Partly Cloudy','Humid and Mostly Cloudy','Humid','Dry','Humid and Partly Cloudy','Possible Light Rain']\n",
    "windy = ['Overcast and Windy','Breezy','Fair and Breezy','Mostly Cloudy and Breezy','Overcast and Breezy','Partly Cloudy and Breezy']\n",
    "snow = [' Blowing Snow and Breezy',' Heavy Snow Freezing Fog',' Light Freezing Rain',' Light Snow',' Light Snow Fog/Mist',' Snow and Breezy','Flurries','Light Snow','Snow','Wintry Mix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt['desc_simple'] = wt.desc.apply(lambda x: 'Good' if x in good else 'Wet' if x in wet else 'Windy' if x in windy else 'Snowy' if x in snow else 'Missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging into ppg master dataframe\n",
    "ppg = ppg.merge(wt, how='left', left_on=['home_team','season_year','week'],right_on=['team_id','Year','Week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppg[ppg['temp'].isnull()]['gsis_id'].nunique()/ppg['gsis_id'].nunique()\n",
    "#missing weather data for 16% of the games\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_weather={}\n",
    "for y in ppg['season_year'].unique():\n",
    "    missing_weather[y]=ppg[(ppg['temp'].isnull())&(ppg['season_year']==y)]['gsis_id'].nunique()/ppg[ppg['season_year']==y]['gsis_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2009: 0,\n",
       " 2010: 1,\n",
       " 2011: 0,\n",
       " 2012: 0,\n",
       " 2013: 0,\n",
       " 2014: 0,\n",
       " 2015: 0,\n",
       " 2016: 0,\n",
       " 2017: 0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Well that explains it - all of 2010 is missing. Confirmed via url that 2010 weather data 404s.\n",
    "missing_weather\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_weather={}\n",
    "for y in ppg['week'].unique():\n",
    "    weekly_weather[y]=ppg[(ppg['temp'].isnull())&(ppg['week']==y)]['gsis_id'].nunique()/ppg[ppg['week']==y]['gsis_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0,\n",
       " 2: 0,\n",
       " 3: 0,\n",
       " 4: 0,\n",
       " 5: 0,\n",
       " 6: 0,\n",
       " 7: 0,\n",
       " 8: 0,\n",
       " 9: 0,\n",
       " 10: 0,\n",
       " 11: 0,\n",
       " 12: 0,\n",
       " 13: 0,\n",
       " 14: 0,\n",
       " 15: 0,\n",
       " 16: 0,\n",
       " 17: 0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#No weekly trend on missing weather.\n",
    "#Could be some PyLibs to find missing weather for a given geo.\n",
    "weekly_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding in rolling averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling=ppg.sort_values(['name_fixed','week']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Previous 3- and 5-week average scores\n",
    "rolling['prev_score'] = rolling.groupby(['season_year','name_fixed'])['fantasy_score'].rolling(1).mean().shift().reset_index(level=[0,1])['fantasy_score']\n",
    "rolling['avg3_score'] = rolling.groupby(['season_year','name_fixed'])['fantasy_score'].rolling(3).mean().shift().reset_index(level=[0,1])['fantasy_score']\n",
    "rolling['avg5_score'] = rolling.groupby(['season_year','name_fixed'])['fantasy_score'].rolling(5).mean().shift().reset_index(level=[0,1])['fantasy_score']\n",
    "rolling['prev_tar']=rolling.groupby(['season_year','name_fixed'])['receiving_tar'].rolling(1).mean().shift().reset_index(level=[0,1])['receiving_tar']\n",
    "rolling['avg3_tar']=rolling.groupby(['season_year','name_fixed'])['receiving_tar'].rolling(3).mean().shift().reset_index(level=[0,1])['receiving_tar']\n",
    "rolling['avg5_tar']=rolling.groupby(['season_year','name_fixed'])['receiving_tar'].rolling(5).mean().shift().reset_index(level=[0,1])['receiving_tar']\n",
    "rolling['prev_rush_att']=rolling.groupby(['season_year','name_fixed'])['rushing_att'].rolling(1).mean().shift().reset_index(level=[0,1])['rushing_att']\n",
    "rolling['avg3_rush_att']=rolling.groupby(['season_year','name_fixed'])['rushing_att'].rolling(3).mean().shift().reset_index(level=[0,1])['rushing_att']\n",
    "rolling['avg5_rush_att']=rolling.groupby(['season_year','name_fixed'])['rushing_att'].rolling(5).mean().shift().reset_index(level=[0,1])['rushing_att']\n",
    "rolling['prev_pass_att']=rolling.groupby(['season_year','name_fixed'])['passing_att'].rolling(1).mean().shift().reset_index(level=[0,1])['passing_att']\n",
    "rolling['avg3_pass_att']=rolling.groupby(['season_year','name_fixed'])['passing_att'].rolling(3).mean().shift().reset_index(level=[0,1])['passing_att']\n",
    "rolling['avg5_pass_att']=rolling.groupby(['season_year','name_fixed'])['passing_att'].rolling(5).mean().shift().reset_index(level=[0,1])['passing_att']\n",
    "%ding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpt = rolling.to_csv('/Users/ianbury/Springboard/FantasyFootball/chckpt1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FanDuel Data and Structure\n",
    "Each team has a salary cap of $60,000 to fill 9 positions [1-QB, 2-RB, 3-WR, 1-TE, 1-FLEX, 1-DEF]. FLEX can be either a WR, RB or TE. Example salary data for preseason 2018 is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd = pd.read_csv('/Users/ianbury/Downloads/FanDuel-NFL-2018-09-09-27409-players-list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd.groupby('Position')['Salary'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd=fd.set_index('Position').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "problem = Problem()\n",
    "problem.addVariable('RB1',fd.loc['RB'].Salary)\n",
    "problem.addVariable('RB2',fd.loc['RB'].Salary)\n",
    "problem.addVariable('QB',fd.loc['QB'].Salary)\n",
    "#problem.addVariable('WR1',fd.loc['WR'].Salary)\n",
    "#problem.addVariable('WR2',fd.loc['WR'].Salary)\n",
    "#problem.addVariable('WR3',fd.loc['WR'].Salary)\n",
    "#problem.addVariable('TE',fd.loc['TE'].Salary)\n",
    "#problem.addVariable('FLEX',fd.loc[['TE','QB','RB']].Salary)\n",
    "#problem.addVariable('D',fd.loc['D'].Salary)\n",
    "problem.addConstraint(MaxSumConstraint(30000))\n",
    "problem.addConstraint(MinSumConstraint(24000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = problem.getSolutions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('k')\n",
    "%ding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
